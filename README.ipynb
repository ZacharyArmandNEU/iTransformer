{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iTransformer\n",
    "\n",
    "The repo is the official implementation for the paper: [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting](https://arxiv.org/abs/2310.06625). [[Slides]](https://cloud.tsinghua.edu.cn/f/175ff98f7e2d44fbbe8e/), [[Poster]](https://cloud.tsinghua.edu.cn/f/36a2ae6c132d44c0bd8c/).\n",
    "\n",
    "\n",
    "# Updates\n",
    "\n",
    ":triangular_flag_on_post: **News** (2024.10) [TimeXer](https://arxiv.org/abs/2402.19072), a Transformer for predicting with exogenous variables, is released. Code is available [here](https://github.com/thuml/TimeXer). \n",
    "\n",
    ":triangular_flag_on_post: **News** (2024.05) Many thanks for the great efforts from [lucidrains](https://github.com/lucidrains/iTransformer). A pip package for the usage of iTransformer variants can be simply installed via ```pip install iTransformer```\n",
    "\n",
    ":triangular_flag_on_post: **News** (2024.04) iTransformer has benn included in [NeuralForecast](https://github.com/Nixtla/neuralforecast/blob/main/neuralforecast/models/itransformer.py). Special thanks to the contributor @[Marco](https://github.com/marcopeix)!\n",
    "\n",
    ":triangular_flag_on_post: **News** (2024.03) Introduction of our work in [Chinese](https://mp.weixin.qq.com/s/-pvBnA1_NSloNxa6TYXTSg) is available.\n",
    "\n",
    ":triangular_flag_on_post: **News** (2024.02) iTransformer has been accepted as **ICLR 2024 Spotlight**.\n",
    "\n",
    ":triangular_flag_on_post: **News** (2023.12) iTransformer available in [GluonTS](https://github.com/awslabs/gluonts/pull/3017) with probablistic head and support for static covariates. Notebook is available [here](https://github.com/awslabs/gluonts/blob/dev/examples/iTransformer.ipynb).\n",
    "\n",
    ":triangular_flag_on_post: **News** (2023.12) We received lots of valuable suggestions. A [revised version](https://arxiv.org/pdf/2310.06625v2.pdf) (**24 Pages**) is now available.\n",
    "\n",
    ":triangular_flag_on_post: **News** (2023.10) iTransformer has been included in [[Time-Series-Library]](https://github.com/thuml/Time-Series-Library) and achieves state-of-the-art in Lookback-$96$ forecasting.\n",
    "\n",
    ":triangular_flag_on_post: **News** (2023.10) All the scripts for the experiments in our [paper](https://arxiv.org/pdf/2310.06625.pdf) are available.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "üåü Considering the characteristics of multivariate time series, iTransformer breaks the conventional structure without modifying any Transformer modules. **Inverted Transformer is all you need in MTSF**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./figures/motivation.png\"  alt=\"\" align=center />\n",
    "</p>\n",
    "\n",
    "üèÜ iTransformer achieves the comprehensive state-of-the-art in challenging multivariate forecasting tasks and solves several pain points of Transformer on extensive time series data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./figures/radar.png\" height = \"360\" alt=\"\" align=center />\n",
    "</p>\n",
    "\n",
    "\n",
    "## Overall Architecture\n",
    "\n",
    "iTransformer regards **independent time series as variate tokens** to **capture multivariate correlations by attention** and **utilize layernorm and feed-forward networks to learn series representations**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./figures/architecture.png\" alt=\"\" align=center />\n",
    "</p>\n",
    "\n",
    "The pseudo-code of iTransformer is as simple as the following:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./figures/algorithm.png\" alt=\"\" align=center />\n",
    "</p>\n",
    "\n",
    "## Usage \n",
    "\n",
    "1. Install Pytorch and necessary dependencies.\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "1. The datasets can be obtained from [Google Drive](https://drive.google.com/file/d/1l51QsKvQPcqILT3DwfjCgx8Dsg2rpjot/view?usp=drive_link) or [Baidu Cloud](https://pan.baidu.com/s/11AWXg1Z6UwjHzmto4hesAA?pwd=9qjr).\n",
    "\n",
    "2. Train and evaluate the model. We provide all the above tasks under the folder ./scripts/. You can reproduce the results as the following examples:\n",
    "\n",
    "```\n",
    "# Multivariate forecasting with iTransformer\n",
    "bash ./scripts/multivariate_forecasting/Traffic/iTransformer.sh\n",
    "\n",
    "# Compare the performance of Transformer and iTransformer\n",
    "bash ./scripts/boost_performance/Weather/iTransformer.sh\n",
    "\n",
    "# Train the model with partial variates, and generalize on the unseen variates\n",
    "bash ./scripts/variate_generalization/ECL/iTransformer.sh\n",
    "\n",
    "# Test the performance on the enlarged lookback window\n",
    "bash ./scripts/increasing_lookback/Traffic/iTransformer.sh\n",
    "\n",
    "# Utilize FlashAttention for acceleration\n",
    "bash ./scripts/efficient_attentions/iFlashTransformer.sh\n",
    "```\n",
    "\n",
    "## Main Result of Multivariate Forecasting\n",
    "\n",
    "We evaluate the iTransformer on challenging multivariate forecasting benchmarks (**generally hundreds of variates**). **Comprehensive good performance** (MSE/MAE $\\downarrow$) is achieved.\n",
    "\n",
    "\n",
    "\n",
    "### Online Transaction Load Prediction of Alipay Trading Platform (Avg Results) \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./figures/main_results_alipay.png\" alt=\"\" align=center />\n",
    "</p>\n",
    "\n",
    "## General Performance Boosting on Transformers\n",
    "\n",
    "By introducing the proposed framework, Transformer and its variants achieve **significant performance improvement**, demonstrating the **generality of the iTransformer approach** and **benefiting from efficient attention mechanisms**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./figures/boosting.png\" alt=\"\" align=center />\n",
    "</p>\n",
    "\n",
    "## Zero-Shot Generalization on Variates\n",
    "\n",
    "**Technically, iTransformer is able to forecast with arbitrary numbers of variables**. We train iTransformers on partial variates and forecast unseen variates with good generalizability.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./figures/generability.png\" alt=\"\" align=center />\n",
    "</p>\n",
    "\n",
    "## Model Analysis\n",
    "\n",
    "Benefiting from inverted Transformer modules: \n",
    "\n",
    "- (Left) Inverted Transformers learn **better time series representations** (more similar [CKA](https://github.com/jayroxis/CKA-similarity)) favored by forecasting.\n",
    "- (Right) The inverted self-attention module learns **interpretable multivariate correlations**.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./figures/analysis.png\" alt=\"\" align=center />\n",
    "</p>\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you find this repo helpful, please cite our paper. \n",
    "\n",
    "```\n",
    "@article{liu2023itransformer,\n",
    "  title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},\n",
    "  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},\n",
    "  journal={arXiv preprint arXiv:2310.06625},\n",
    "  year={2023}\n",
    "}\n",
    "```\n",
    "\n",
    "## Acknowledgement\n",
    "\n",
    "We appreciate the following GitHub repos a lot for their valuable code and efforts.\n",
    "- Reformer (https://github.com/lucidrains/reformer-pytorch)\n",
    "- Informer (https://github.com/zhouhaoyi/Informer2020)\n",
    "- FlashAttention (https://github.com/shreyansh26/FlashAttention-PyTorch)\n",
    "- Autoformer (https://github.com/thuml/Autoformer)\n",
    "- Stationary (https://github.com/thuml/Nonstationary_Transformers)\n",
    "- Time-Series-Library (https://github.com/thuml/Time-Series-Library)\n",
    "- lucidrains (https://github.com/lucidrains/iTransformer)\n",
    "\n",
    "This work was supported by Ant Group through the CCF-Ant Research Fund, and awarded as [Outstanding Projects of CCF Fund](https://mp.weixin.qq.com/s/PDLNbibZD3kqhcUoNejLfA).\n",
    "\n",
    "## Contact\n",
    "\n",
    "If you have any questions or want to use the code, feel free to contact:\n",
    "* Yong Liu (liuyong21@mails.tsinghua.edu.cn)\n",
    "* Haoran Zhang (z-hr20@mails.tsinghua.edu.cn)\n",
    "* Tengge Hu (htg21@mails.tsinghua.edu.cn)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
